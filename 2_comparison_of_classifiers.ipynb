{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbef5b5",
   "metadata": {},
   "source": [
    "In this assignment I am to create both a decision tree classifier and a naive bayes classifier for a breast cancer prediction dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb898541",
   "metadata": {},
   "source": [
    "Wisconsin Diagnostic Breast Cancer dataset is a binary class dataset whose primary task is to classify whether the breast tumor is malignant or benign. Each observation in the dataset corresponds to one patient case and contains a set of measurements that were obtained through digitized images of a fine needle aspirate of a breast mass. The measurements report various properties of the cell nuclei in the sample.\n",
    "\n",
    "There are thirty numerical attributes per case in the database, measuring statistical properties of nuclei such as their radius, perimeter, area, texture, smoothness, compactness, concavity, symmetry and fractal dimension. For each of these measurements, the mean value, standard error and a \"worst\" or maximum value are noted, providing an accurate description of the sample. Aside from these features, there is a single identifier column that simply labels the samples and does not have any predictive information. The column is typically not used at the preprocessing stage because it will not be helpful when classifying.\n",
    "\n",
    "The target feature is the diagnosis column, which marks whether the tumor is malignant or benign. Malignant tumors are labeled with the character M and benign tumors with the character B. The objective of a machine learning algorithm trained on this data is therefore to acquire a decision function that, from the thirty numeric features, can correctly determine whether a tumor is cancerous. The purpose of the dataset is intended in terms of enabling the development and evaluation of classification methods that may be applied for helping early diagnosis of breast cancer and guiding clinical decision making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdaf29f",
   "metadata": {},
   "source": [
    "Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6269d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (569, 32)\n",
      "Diagnosis counts:\n",
      " diagnosis\n",
      "B    357\n",
      "M    212\n",
      "Name: count, dtype: int64\n",
      "Missing values: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave_points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "#loading data\n",
    "path = r\"C:\\Users\\gumas\\OneDrive\\Skrivebord\\Dataingenior\\Programmer\\Visual Studio Code\\Mandatory assignment 1\\data\\wdbc.data\"\n",
    "df = pd.read_csv(path, header=None)\n",
    "\n",
    "#I assign the column names\n",
    "# From the wdbc.names file: \"field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\":\n",
    "colnames = (\n",
    "    [\"id\", \"diagnosis\", \n",
    "        \"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\n",
    "        \"compactness_mean\",\"concavity_mean\",\"concave_points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\n",
    "        \"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\n",
    "        \"compactness_se\",\"concavity_se\",\"concave_points_se\",\"symmetry_se\",\"fractal_dimension_se\",\n",
    "        \"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\n",
    "        \"compactness_worst\",\"concavity_worst\",\"concave_points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"\n",
    "    ]\n",
    ")\n",
    "df.columns = colnames\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Diagnosis counts:\\n\", df[\"diagnosis\"].value_counts())\n",
    "\n",
    "#Missing values is 0. This fits the description in the dataset info file\n",
    "print(\"Missing values:\", df.isna().sum().sum())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c7cde",
   "metadata": {},
   "source": [
    "In this step, I will preform the necassary pre processing of the data; removing the unstable columns. An unstable column is a column that does not have any useful or predictive information for the classification task. In this dataset, the ID- set is an example of this. It contains only a rndom identifier for each sample. This identifier is unique for each row and has no correlation to whether the tumor is malignant or benign. If I were to include it, it would mislead the model into memorizing arbitary patterns. Therefore, I chose to drop the id column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80b344",
   "metadata": {},
   "source": [
    "Step 2: Pre-processing - removing the unstable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (569, 30) | y shape: (569,)\n",
      "Class balance (0=benign, 1=malignant): [357 212]\n"
     ]
    }
   ],
   "source": [
    "# dropping non-predictive ID\n",
    "df_clean = df.drop(columns=[\"id\"]).copy()\n",
    "\n",
    "#  target: malignant=1, benign=0\n",
    "df_clean[\"target\"] = (df_clean[\"diagnosis\"] == \"M\").astype(int)\n",
    "df_clean = df_clean.drop(columns=[\"diagnosis\"])\n",
    "\n",
    "# eatures/labels\n",
    "X = df_clean.drop(columns=[\"target\"]).values\n",
    "y = df_clean[\"target\"].values\n",
    "\n",
    "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n",
    "print(\"Class balance (0=benign, 1=malignant):\", np.bincount(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143978a",
   "metadata": {},
   "source": [
    "Regarding the testing of the models on the unseen data, I divided the data into a test set and a training set using random sampling. I used a 80/20 split, which is a common practice and it provides enough data for the model to learn great patterns and yet has a big enough hold-out set ffor an honest test. The division was stratified on the target feature to perserve the original ratio of malignant and benign cases for each of the divisions. Doing this helps prevent class imbalances in each set and it enables an equal comparison of performance measures. The reproductibility of my results was ensured using a fixed random state.\n",
    "\n",
    "I partitioned the data using stratified random sampling. This random sampling avoids any ordering bias which may have been prevalent in the original data. To keep the same proportion of malignant and benign cases within the training- and test set, I did the stratification on the basis of the target variable. Doing this ensured that both sets represented the overall population and the performance measurements are not biased because of this class imbalance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb2b31",
   "metadata": {},
   "source": [
    "Step 3: Stratified random split (defense: 80/20, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169cd73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'neg(0)': 285, 'pos(1)': 170} size: 455\n",
      "Test : {'neg(0)': 72, 'pos(1)': 42} size: 114\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "def counts(arr):\n",
    "    return {\"neg(0)\": int((arr==0).sum()), \"pos(1)\": int((arr==1).sum())}\n",
    "\n",
    "print(\"Train:\", counts(y_train), \"size:\", len(y_train))\n",
    "print(\"Test :\", counts(y_test),  \"size:\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc07fda",
   "metadata": {},
   "source": [
    "Step 4: Helper - evaluation on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd6eac",
   "metadata": {},
   "source": [
    "This function is a small helper that is used after training a model to evaluate how well it performs on the test set. The function takes four inputs; the trained model, test features Xte, the best labels yte and an optional name. \n",
    "\n",
    "The model will make predictions on the test data with model.predict(Xte), which will give ywhat, the predicted labels. Then it will calculate different significant performance metrics. The accuracy will be the proportion of all predictions that were correct. The precision emasures how many of the samles predicted as malignant were actually malignant, which is important for understanding the false- positive rate. The recall measures how many of the truly malignant cases where successfully detected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e14f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, Xte, yte, name=\"model\"):\n",
    "    yhat = model.predict(Xte)\n",
    "    acc = accuracy_score(yte, yhat)\n",
    "    prec = precision_score(yte, yhat, zero_division=0)\n",
    "    rec = recall_score(yte, yhat, zero_division=0)\n",
    "    f1  = f1_score(yte, yhat, zero_division=0)\n",
    "    cm  = confusion_matrix(yte, yhat)\n",
    "    print(f\"\\n{name} — Test metrics\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f} (positive=malignant)\")\n",
    "    print(f\"Recall   : {rec:.4f} (positive=malignant)\")\n",
    "    print(f\"F1-score : {f1:.4f} (positive=malignant)\")\n",
    "    print(\"Confusion matrix [[TN, FP],[FN, TP]]:\\n\", cm)\n",
    "    print(\"\\nClassification report:\\n\", classification_report(yte, yhat, digits=4))\n",
    "    return {\"acc\":acc,\"prec\":prec,\"rec\":rec,\"f1\":f1,\"cm\":cm}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f38a88",
   "metadata": {},
   "source": [
    "Instead of manually applying different values for the hyperparameter tuning, I used nested for each loops to find the best results and store the values in a list.\n",
    "\n",
    "This code performs manual hyperparameter tuning for the decision tree classifier. It first makes a StratifiedKFold object that splits the training set into five folds with the same ratio of malignant and benign cases in all the folds. It then runs over all possible combinations of four critical hyperparameters; the splitting rule, maximum tree depth, minimum samples to split a node, and minimum samples in a leaf. For each combination, it builds a  decision tree and cross-validates it, calculating the mean and standard deviation of the F1-score across the five folds. The result of each combination is stored as dictionaries in a list.\n",
    "\n",
    "Once the loops are done, the result list is converted to a pandas DataFrame and sorted by the mean F1-score so that the best-performing combinations are shown first. The top ten rows of this sorted DataFrame are then printed so that you can see which hyperparameter setting performed best. In the second step of the code, the optimal combination, the first row of the sorted DataFrame is obtained and used to train the last decision tree model on the entire training set prior to testing it on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03d22e",
   "metadata": {},
   "source": [
    "Step 5: Manual tuning using nested for each loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b45fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>cv_f1_mean</th>\n",
       "      <th>cv_f1_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>entropy</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.916303</td>\n",
       "      <td>0.034097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>entropy</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.916303</td>\n",
       "      <td>0.034097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>entropy</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.914479</td>\n",
       "      <td>0.025547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>entropy</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.914191</td>\n",
       "      <td>0.026295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>entropy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.912127</td>\n",
       "      <td>0.034256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>entropy</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.910777</td>\n",
       "      <td>0.032087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.909411</td>\n",
       "      <td>0.034930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>entropy</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.909411</td>\n",
       "      <td>0.034930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>entropy</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.909411</td>\n",
       "      <td>0.034930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>entropy</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.908680</td>\n",
       "      <td>0.026739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    criterion  max_depth  min_samples_split  min_samples_leaf  cv_f1_mean  \\\n",
       "67    entropy        4.0                  5                 2    0.916303   \n",
       "64    entropy        4.0                  2                 2    0.916303   \n",
       "63    entropy        4.0                  2                 1    0.914479   \n",
       "70    entropy        4.0                 10                 2    0.914191   \n",
       "79    entropy        5.0                 10                 2    0.912127   \n",
       "66    entropy        4.0                  5                 1    0.910777   \n",
       "106   entropy        NaN                 10                 2    0.909411   \n",
       "88    entropy        6.0                 10                 2    0.909411   \n",
       "97    entropy        8.0                 10                 2    0.909411   \n",
       "69    entropy        4.0                 10                 1    0.908680   \n",
       "\n",
       "     cv_f1_std  \n",
       "67    0.034097  \n",
       "64    0.034097  \n",
       "63    0.025547  \n",
       "70    0.026295  \n",
       "79    0.034256  \n",
       "66    0.032087  \n",
       "106   0.034930  \n",
       "88    0.034930  \n",
       "97    0.034930  \n",
       "69    0.026739  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "dt_results = []\n",
    "for criterion in [\"gini\", \"entropy\"]:\n",
    "    for max_depth in [3, 4, 5, 6, 8, None]:\n",
    "        for min_samples_split in [2, 5, 10]:\n",
    "            for min_samples_leaf in [1, 2, 4]:\n",
    "                dt_temp = DecisionTreeClassifier(\n",
    "                    criterion=criterion,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    random_state=42\n",
    "                )\n",
    "                # using cross-validated F1 on training data only\n",
    "                scores = cross_val_score(dt_temp, X_train, y_train, cv=cv, scoring=\"f1\")\n",
    "                dt_results.append({\n",
    "                    \"criterion\": criterion,\n",
    "                    \"max_depth\": max_depth,\n",
    "                    \"min_samples_split\": min_samples_split,\n",
    "                    \"min_samples_leaf\": min_samples_leaf,\n",
    "                    \"cv_f1_mean\": scores.mean(),\n",
    "                    \"cv_f1_std\":  scores.std()\n",
    "                })\n",
    "\n",
    "dt_results_df = pd.DataFrame(dt_results).sort_values(\"cv_f1_mean\", ascending=False)\n",
    "dt_results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e46e6",
   "metadata": {},
   "source": [
    "Step 6: Train the chosen Decision Tree and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5adde3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "criterion             entropy\n",
       "max_depth                 4.0\n",
       "min_samples_split           5\n",
       "min_samples_leaf            2\n",
       "cv_f1_mean           0.916303\n",
       "cv_f1_std            0.034097\n",
       "Name: 67, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the first row in the sorted dataframe because it contains the best combination of hyperparameters\n",
    "best_dt = dt_results_df.iloc[0]\n",
    "best_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78457d2b",
   "metadata": {},
   "source": [
    "This code creates the final decision tree using the best hyperparameters found in the previous tuning step, trains it on the entire training set, and then evaluates its performance on the test set using the evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1137b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree (manual tuned) — Test metrics\n",
      "Accuracy : 0.9298\n",
      "Precision: 1.0000 (positive=malignant)\n",
      "Recall   : 0.8095 (positive=malignant)\n",
      "F1-score : 0.8947 (positive=malignant)\n",
      "Confusion matrix [[TN, FP],[FN, TP]]:\n",
      " [[72  0]\n",
      " [ 8 34]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9000    1.0000    0.9474        72\n",
      "           1     1.0000    0.8095    0.8947        42\n",
      "\n",
      "    accuracy                         0.9298       114\n",
      "   macro avg     0.9500    0.9048    0.9211       114\n",
      "weighted avg     0.9368    0.9298    0.9280       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_best_manual = DecisionTreeClassifier(\n",
    "    criterion=best_dt[\"criterion\"],\n",
    "    max_depth=None if pd.isna(best_dt[\"max_depth\"]) else int(best_dt[\"max_depth\"]) if best_dt[\"max_depth\"] is not None else None,\n",
    "    min_samples_split=int(best_dt[\"min_samples_split\"]),\n",
    "    min_samples_leaf=int(best_dt[\"min_samples_leaf\"]),\n",
    "    random_state=42\n",
    ")\n",
    "dt_best_manual.fit(X_train, y_train)\n",
    "dt_test = evaluate(dt_best_manual, X_test, y_test, name=\"Decision Tree (manual tuned)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6cc4d",
   "metadata": {},
   "source": [
    "Step 7:  Manual tuning GassianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067627d0",
   "metadata": {},
   "source": [
    "In this code, an empty list is created, and the program loops through several values of the var_smoothing parameter for gaussive naive bayes, and a temporare model is trained for each value, while its mean F1 score is measured using cross validation.\n",
    "\n",
    "each result gets stored in the list, which then gets converted into a dataframe and sorted so tat the highest performing value is at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ca149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_smoothing</th>\n",
       "      <th>cv_f1_mean</th>\n",
       "      <th>cv_f1_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.994843e-10</td>\n",
       "      <td>0.918514</td>\n",
       "      <td>0.036146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>0.917039</td>\n",
       "      <td>0.034203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.593814e-12</td>\n",
       "      <td>0.917039</td>\n",
       "      <td>0.034203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.291550e-11</td>\n",
       "      <td>0.917039</td>\n",
       "      <td>0.034203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.641589e-11</td>\n",
       "      <td>0.913809</td>\n",
       "      <td>0.037602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.668101e-10</td>\n",
       "      <td>0.913724</td>\n",
       "      <td>0.035581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.742637e-09</td>\n",
       "      <td>0.910204</td>\n",
       "      <td>0.036487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.154435e-09</td>\n",
       "      <td>0.907261</td>\n",
       "      <td>0.041180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.901795</td>\n",
       "      <td>0.033775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.782559e-08</td>\n",
       "      <td>0.896033</td>\n",
       "      <td>0.037920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var_smoothing  cv_f1_mean  cv_f1_std\n",
       "5   5.994843e-10    0.918514   0.036146\n",
       "0   1.000000e-12    0.917039   0.034203\n",
       "1   3.593814e-12    0.917039   0.034203\n",
       "2   1.291550e-11    0.917039   0.034203\n",
       "3   4.641589e-11    0.913809   0.037602\n",
       "4   1.668101e-10    0.913724   0.035581\n",
       "7   7.742637e-09    0.910204   0.036487\n",
       "6   2.154435e-09    0.907261   0.041180\n",
       "9   1.000000e-07    0.901795   0.033775\n",
       "8   2.782559e-08    0.896033   0.037920"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_results = []\n",
    "for vs in np.logspace(-12, -7, 10):\n",
    "    nb_temp = GaussianNB(var_smoothing=vs)\n",
    "    scores = cross_val_score(nb_temp, X_train, y_train, cv=cv, scoring=\"f1\")\n",
    "    nb_results.append({\n",
    "        \"var_smoothing\": vs,\n",
    "        \"cv_f1_mean\": scores.mean(),\n",
    "        \"cv_f1_std\":  scores.std()\n",
    "    })\n",
    "\n",
    "nb_results_df = pd.DataFrame(nb_results).sort_values(\"cv_f1_mean\", ascending=False)\n",
    "nb_results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072912f5",
   "metadata": {},
   "source": [
    "Step 8: Train the chosen GaussianNB and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf38dfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var_smoothing    5.994843e-10\n",
       "cv_f1_mean       9.185135e-01\n",
       "cv_f1_std        3.614565e-02\n",
       "Name: 5, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the the first row in the sorted dataframe because it contains the best combination of hyperparameters\n",
    "best_nb = nb_results_df.iloc[0]\n",
    "best_nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a7a68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GaussianNB (manual tuned) — Test metrics\n",
      "Accuracy : 0.9386\n",
      "Precision: 0.9730 (positive=malignant)\n",
      "Recall   : 0.8571 (positive=malignant)\n",
      "F1-score : 0.9114 (positive=malignant)\n",
      "Confusion matrix [[TN, FP],[FN, TP]]:\n",
      " [[71  1]\n",
      " [ 6 36]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9221    0.9861    0.9530        72\n",
      "           1     0.9730    0.8571    0.9114        42\n",
      "\n",
      "    accuracy                         0.9386       114\n",
      "   macro avg     0.9475    0.9216    0.9322       114\n",
      "weighted avg     0.9408    0.9386    0.9377       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_best_manual = GaussianNB(var_smoothing=float(best_nb[\"var_smoothing\"]))\n",
    "nb_best_manual.fit(X_train, y_train)\n",
    "nb_test = evaluate(nb_best_manual, X_test, y_test, name=\"GaussianNB (manual tuned)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047f887",
   "metadata": {},
   "source": [
    "Step 9: Side-by-side comparison table (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "106f99dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.911392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Algorithm  Accuracy  Precision    Recall        F1\n",
       "0     GaussianNB  0.938596   0.972973  0.857143  0.911392\n",
       "1  Decision Tree  0.929825   1.000000  0.809524  0.894737"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp = pd.DataFrame({\n",
    "    \"Algorithm\": [\"Decision Tree\", \"GaussianNB\"],\n",
    "    \"Accuracy\":  [dt_test[\"acc\"], nb_test[\"acc\"]],\n",
    "    \"Precision\": [dt_test[\"prec\"], nb_test[\"prec\"]],\n",
    "    \"Recall\":    [dt_test[\"rec\"], nb_test[\"rec\"]],\n",
    "    \"F1\":        [dt_test[\"f1\"], nb_test[\"f1\"]],\n",
    "})\n",
    "comp.sort_values(\"F1\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9038e2",
   "metadata": {},
   "source": [
    "**Discussing the results**\n",
    "\n",
    "In this task, I would say that both of the classifiers (GaussianNB, Decision tree) achieved great performances on the breast cancer dataset. Both of them had high accuracies above 92%; GaussianNB with 93.8% and Decision Tree with 92.9%. GaussianNB had a higher recall and F1 score as well, while Decision Tree managed to get a perfect 1 score. Having a perfect 1 score means that every tumor it had classified as malignant indeed was malignant. A consequence of this however is that the recall score turned out to be lower, meaning it missed more malignant cases than GaussianNB did.\n",
    "\n",
    "The main goal of the model is to correctly identify as many malignant cases as possible, making the recall a cruical value for this specific task. GaussianNB had a higher reccall, which in turn resulted in a better F1- score that reflects a more favorable balance between precision and recall. Because of this, GaussianNB is the better algorithm for this specific dataset, because it offers a slightly higher overall performance and is more effective at minimizing false negatives, which is severely important in a medical context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
